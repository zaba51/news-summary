{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install morfeusz2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATW6aDES30sI",
        "outputId": "926f5828-d06a-4592-81a6-4f043c692efa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting morfeusz2\n",
            "  Downloading morfeusz2-1.99.12-cp36-abi3-manylinux_2_28_x86_64.whl.metadata (528 bytes)\n",
            "Downloading morfeusz2-1.99.12-cp36-abi3-manylinux_2_28_x86_64.whl (9.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: morfeusz2\n",
            "Successfully installed morfeusz2-1.99.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from collections import Counter\n",
        "import morfeusz2\n",
        "morf = morfeusz2.Morfeusz()\n",
        "\n",
        "# Scraper\n",
        "import string\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt_tab')\n",
        "# nlp = spacy.load(\"pl_spacy_model_morfeusz\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgwej2F3Le8C",
        "outputId": "17379f36-5806-44b6-a7f8-9ad38fdad92e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_text_from_url(url: string):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        h1_title = soup.find('h1', class_='title').get_text() if soup.find('h1', class_='title') else 'No title found'\n",
        "        p_lead = soup.find('p', class_='lead').get_text() if soup.find('p', class_='lead') else 'No lead found'\n",
        "        div_content_parts = soup.find('div', class_='contentparts')\n",
        "        p_content_parts = []\n",
        "        if div_content_parts:\n",
        "            for p in div_content_parts.find_all('p', class_='contentpart--text'):\n",
        "                text = p.get_text()\n",
        "                if '\\xa0' not in text and 'ZOBACZ WIDEO' not in text and not (p.find('a') and text == p.find('a').get_text()):\n",
        "                    p_content_parts.append(text)\n",
        "        return h1_title, p_lead, p_content_parts\n",
        "    else:\n",
        "        return None, 'No title found', 'No lead found'"
      ],
      "metadata": {
        "id": "05hvC2foMiU_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def find_first_match(word, analyses, verbose=False):\n",
        "    if (verbose):\n",
        "      print(word, analyses)\n",
        "\n",
        "    for analysis in analyses:\n",
        "        if analysis[2][0] == word:\n",
        "            return analysis[2][1].split(\":\")[0]\n",
        "    return word\n",
        "\n",
        "def preprocess_text(text, lematize=True):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    text = re.sub(r\"[(),]\", \"\", text).strip()\n",
        "\n",
        "    if (lematize):\n",
        "      lematized = [ find_first_match(word, morf.analyse(word)) for word in text.split()]\n",
        "      print(lematized)\n",
        "      return \" \".join(lematized)\n",
        "    return text"
      ],
      "metadata": {
        "id": "11gf1ra3tWgZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_text(\"Coś się stało ciekawego i teraz ten tekst miałby lub takżę będzie lematyzowany Luke ja jestem twoim ojcem Litwo ojczyzno moja \")\n",
        "# morf.analyse(\"20:40\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "GadU38TD4W_P",
        "outputId": "d821d0e1-fc5e-4793-82ba-99ecf4e00d07"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['coś', 'się', 'stać', 'ciekawy', 'i', 'teraz', 'ten', 'tekst', 'miałby', 'lub', 'takżę', 'być', 'lematyzowany', 'luke', 'ja', 'być', 'twoi', 'ojciec', 'litwa', 'ojczyzna', 'moja']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'coś się stać ciekawy i teraz ten tekst miałby lub takżę być lematyzowany luke ja być twoi ojciec litwa ojczyzna moja'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_article(url):\n",
        "    res = scrape_text_from_url(url)\n",
        "    raw_text = '. '.join((res[0], res[1], ' '.join(res[2])) )\n",
        "\n",
        "    original_sentences = raw_text.split(\". \")\n",
        "\n",
        "    processed_sentences = [preprocess_text(sentence).split(\" \") for sentence in original_sentences if sentence.strip()]\n",
        "\n",
        "    return original_sentences, processed_sentences\n",
        "\n",
        "    # -------------------------------------------\n",
        "    # article = text.split(\". \")\n",
        "    # sentences = []\n",
        "    # for sentence in article:\n",
        "    #     sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
        "    # return sentences\n",
        "    # -------------------------------------------\n",
        "    # sentences = sent_tokenize(text)  # Tokenizacja zdań\n",
        "    # return [word_tokenize(sentence) for sentence in sentences]"
      ],
      "metadata": {
        "id": "CcdxyiV2LlmM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "isoTt3T4Lctt"
      },
      "outputs": [],
      "source": [
        "def sentence_similarity(sent1, sent2):\n",
        "    stop_words = stopwords.words('english')\n",
        "\n",
        "    filtered_sent1 = [word.lower() for word in sent1 if word.lower() not in stop_words]\n",
        "    filtered_sent2 = [word.lower() for word in sent2 if word.lower() not in stop_words]\n",
        "\n",
        "    word_counts1 = Counter(filtered_sent1)\n",
        "    word_counts2 = Counter(filtered_sent2)\n",
        "\n",
        "    all_words = set(word_counts1.keys()).union(set(word_counts2.keys()))\n",
        "\n",
        "    vector1 = [word_counts1.get(word, 0) for word in all_words]\n",
        "    vector2 = [word_counts2.get(word, 0) for word in all_words]\n",
        "\n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        "\n",
        "# def sentence_similarity(sent1, sent2):\n",
        "#     stop_words = stopwords.words('english')\n",
        "\n",
        "#     sent1 = [w.lower() for w in sent1]\n",
        "#     sent2 = [w.lower() for w in sent2]\n",
        "\n",
        "#     all_words = list(set(sent1 + sent2))\n",
        "\n",
        "#     vector1 = [0] * len(all_words)\n",
        "#     vector2 = [0] * len(all_words)\n",
        "\n",
        "#     # zbudowanie wektora dla pierwszego zdania\n",
        "#     for w in sent1:\n",
        "#         if w in stop_words:\n",
        "#             continue\n",
        "#         vector1[all_words.index(w)] += 1\n",
        "\n",
        "#     # zbudowanie wektora dla drugiego zdania\n",
        "#     for w in sent2:\n",
        "#         if w in stop_words:\n",
        "#             continue\n",
        "#         vector2[all_words.index(w)] += 1\n",
        "\n",
        "#     return 1 - cosine_distance(vector1, vector2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_matrix_bow(sentences):\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "\n",
        "    for i in range(len(sentences)):\n",
        "        for j in range(len(sentences)):\n",
        "            if i != j:\n",
        "              similarity_matrix[i][j] = sentence_similarity(sentences[i], sentences[j])\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "\n",
        "# #  Alternatywna metoda\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "def similarity_matrix_tfidf(sentences, stop_words = stopwords.words('english')):\n",
        "    # Połącz zdania w jeden tekst na potrzeby TF-IDF\n",
        "    sentence_texts = [\" \".join(sentence) for sentence in sentences]\n",
        "\n",
        "    # Stwórz macierz TF-IDF\n",
        "    tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(sentence_texts)\n",
        "\n",
        "    # Wyznacz podobieństwo kosinusowe między wektorami TF-IDF\n",
        "    similarity_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()\n",
        "    return similarity_matrix"
      ],
      "metadata": {
        "id": "9iZchagdLu1B"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kernel = 'bow'\n",
        "\n",
        "def generate_summary(url, top_n=5):\n",
        "    # Krok 1 - Odczytanie oryginalnych i przetworzonych zdań\n",
        "    original_sentences, processed_sentences = read_article(url)\n",
        "    if kernel == 'tfidf':\n",
        "        sentence_similarity_martix = similarity_matrix_tfidf(processed_sentences)\n",
        "    elif kernel == 'bow':\n",
        "        sentence_similarity_martix = similarity_matrix_bow(processed_sentences)\n",
        "    else:\n",
        "        raise Exception(\"Unknown kernel\")\n",
        "\n",
        "    # Krok 3 - Uporządkowanie zdań w macierzy podobieństwa\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    # Sortowanie zdań według wyniku PageRank\n",
        "    ranked_sentences = sorted(((scores[i], i) for i in range(len(processed_sentences))), reverse=True)\n",
        "\n",
        "    # Krok 4 - Wybranie najlepszych zdań w oryginalnej formie\n",
        "    num_sentences = min(top_n, len(ranked_sentences))\n",
        "    summarize_text = [original_sentences[ranked_sentences[i][1]] for i in range(num_sentences)]\n",
        "\n",
        "    # Zwrot oryginalnych zdań jako podsumowanie\n",
        "    return \". \".join(summarize_text)\n"
      ],
      "metadata": {
        "id": "uNC1ZivhLz4z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://sportowefakty.wp.pl/pilka-reczna/1160547/polki-w-meczu-ze-szwecja-podjely-ryzyko-niesamowite-co-zrobila-bramkarka'\n",
        "generate_summary(url, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "vt5_DaaeMrSa",
        "outputId": "768d355c-1127-44ef-c09d-0b46f99fcbaa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['były', 'tylko', 'moment']\n",
            "['bolesny', 'lekcja', 'dla', 'polka', 'na', 'mój']\n",
            "['nie', 'udać', 'się', 'sprawić', 'niespodzianka', 'Polska', 'w', 'pierwszy', 'mecz', 'faza', 'główny', 'mój', '2024', 'piłkarka', 'ręczny']\n",
            "['w', 'pierwszy', 'połowa', 'biało-czerwone', 'walczyć', 'z', 'Szwecja', 'jak', 'równy', 'z', 'równe', 'ale', 'późno', 'do', 'głos', 'dojść', 'rywalka']\n",
            "['z', 'on', 'kapitalny', 'bramkarka', 'na', 'czele.']\n",
            "['reprezentacja', 'polski', 'piłkarka', 'ręczny', 'faza', 'główny', 'mój', '2024', 'do', 'który', 'awansować', 'po', '10', 'lato', 'przerwa', 'rozpocząć', 'od', 'starcie', 'z', 'czwarty', 'siła', 'na', 'świat']\n",
            "['Szwedka', 'czyli', 'półfinalistka', 'ostatni', 'MŚ', 'i', 'io', '2024', 'w', 'Paryż', 'były', 'zdecydować', 'faworyt', 'ale', 'polka', 'nie', 'przestraszyć', 'się', 'rywalka']\n",
            "['przynajmniej', 'w', 'pierwszy', 'połowa']\n",
            "['możny', 'powiedzieć', 'że', 'być', 'to', 'stare', 'dobre', 'znajoma', 'ponieważ', 'pojedynek', 'w', 'Debreczyn', 'być', 'już', 'on', 'starcie', 'nr', '50']\n",
            "['jubileuszowy', 'spotkanie', 'polka', 'rozpocząć', 'od', 'falstart', '5:1', 'w', \"8'\", 'i', 'arne', 'senstad', 'natychmiast', 'poprosić', 'o', 'czas']\n",
            "['wprowadzić', 'zmiana', 'uporządkować', 'zespół', 'i', 'znów', 'podejmowaliśmy', 'ryzyko', 'grać', '7', 'na', '6']\n",
            "['i', 'to', 'przynieść', 'wymierny', 'efekt']\n",
            "['biało-czerwone', 'rzucić', 'się', 'do', 'odrabiać', 'strata', 'i', 'w', 'końcówka', 'pierwszy', 'kwadrans', 'na', 'tablica', 'wynik', 'widnieć', 'już', 'remis']\n",
            "['od', 'to', 'moment', 'toczyliśmy', 'z', 'zespół', 'trzy', 'korona', 'bardzo', 'zaciąć', 'bój']\n",
            "['pewne', 'element', 'zaskoczenie', 'była', 'tenże', 'zmiana', 'bramkarka', '-', 'kilka', 'raz', 'w', 'trakt', 'pierwszy', 'połowa']\n",
            "['zarówno', 'paulin', 'Wdowiak', 'jak', 'tenże', 'Barbara', 'zima', 'spisywać', 'się', 'bardzo', 'dobrze']\n",
            "['a', 'w', 'ataku?', 'ciężar', 'na', 'swój', 'bark', 'brać', 'Monika', 'kobyliński', 'wspierać', 'przez', 'skuteczny', 'skrzydłowy']\n",
            "['dzięki', 'Dagmara', 'Nocuń', 'nasze', 'zawodniczka', 'po', 'raz', 'pierwszy', 'wyjść', 'na', 'prowadzić', '11:12', 'w', \"24'\"]\n",
            "['radość', 'nie', 'trwać', 'jeden', 'długo', 'ponieważ', 'nie', 'do', 'zatrzymanie', 'była', 'nathalie', 'hagman', 'który', 'do', 'przerwa', 'zdobyć', 'aż', 'siedem', 'bramka']\n",
            "['nie', 'sposób', 'tenże', 'nie', 'wspomnieć', 'o', 'bramkarka', 'johannie', 'bundsen']\n",
            "['w', 'końcówka', 'dwa', 'raz', 'rzucić', 'przez', 'cały', 'boisko', 'do', 'pusty', 'bramka', 'i', 'dzięki', 'on', 'Szwedka', 'zeszły', 'do', 'szatnia', 'z', 'dwubramkowy', 'zapas']\n",
            "['i', 'to', 'być', 'moment', 'który', 'odmienić', 'ten', 'mecz']\n",
            "['johanna', 'bundsen', 'wejść', 'na', 'swój', 'znakomity', 'poziom', 'i', 'po', 'zmiana', 'strona', 'dołożyć', 'od', 'siebie', 'już', 'trzeci', 'bramka', '20:16', 'w', \"34'\"]\n",
            "['w', 'ofensywa', 'szaleć', 'jamina', 'Roberts', 'który', 'mieć', 'bardzo', 'dużo', 'swoboda']\n",
            "['wciąż', 'ogromny', 'zagrozić', 'w', 'atak', 'stanowić', 'nathalie', 'hagman']\n",
            "['gdy', 'Szwedka', 'wyjść', 'na', '+7', 'arne', 'senstad', 'poprosić', 'o', 'kolejny', 'czas', '24:17', 'w', \"41'\"]\n",
            "['\"czy', 'wy', 'się', 'już', 'poddałyście?\"', '-', 'próbować', 'przemówić', 'do', 'swoje', 'zawodniczka', 'selekcjoner']\n",
            "['polka', 'podejmować', 'próba', 'ale', 'do', 'to', 'mecz', 'już', 'nie', 'wrócić']\n",
            "['w', 'swój', 'dobry', 'moment', 'gdy', 'piłka', 'do', 'siatka', 'władować', 'córka', 'selekcjoner', '-', 'tyrać', 'axner', 'trzy', 'korona', 'prowadzić', 'już', 'nawet', 'różnica', 'dziesięć', 'bramka', '32:22', 'w', \"56'\"]\n",
            "['biało-czerwonym', 'nie', 'pozostać', 'już', 'nic', 'inny', 'jak', 'zmniejszenie', 'rozmiar', 'porażka', 'co', 'tenże', 'uczynić']\n",
            "['kolejny', 'rywal?', 'węgier']\n",
            "['spotkanie', 'odbyć', 'się', '6', 'grudzień', 'piątek', 'o', 'godzina', '20:30']\n",
            "['mój', '2024:', 'szwecja:', 'bundsen', '3', 'ryde', '-', 'lerby', '3', 'strömberg', 'blohm', 'Roberts', '6', 'axnér', '2', 'dać', 'lindqvist', '4', 'hagman', '9', 'thorleifsdóttir', 'hvenfelt', 'hansson', '2', 'Karlsson', '1', 'löfqvist', '2', 'petersson', 'bergsten', '1.karne:', '2/3kary:', '8', 'mina']\n",
            "['polska:', 'płaczek', 'zima', 'Wdowiak', '-', 'Olka', 'kobyliński', '6', 'Matuszczyk', 'górny', '1', 'Rosiak', '2', 'Drażyk', '1', 'balsam', '2', 'Urbańska', '2', 'nosek', '1', 'Michalak', '1', 'kochaniak-sala', '4', 'Uścinowicz', '1', 'Nocuń', '4.karne:', '0/1kary:', '12', 'min.czerwona', 'kartka:', 'Urbańska', 'w', \"49'\", 'za', 'faul']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Nie udało się sprawić niespodzianki Polsce w pierwszym meczu fazy głównej ME 2024 piłkarek ręcznych. Biało-Czerwone rzuciły się do odrabiania strat i w końcówce pierwszego kwadransa na tablicy wyników widniał już remis. W swoim najlepszym momencie, gdy piłkę do siatki władowała córka selekcjonera - Tyra Axner, Trzy Korony prowadziły już nawet różnicą dziesięciu bramek (32:22 w 56'). Przynajmniej w pierwszej połowie\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"2 4\".replace(\"[^a-zA-Z]\", \" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NVOFgbOUph6n",
        "outputId": "1d5a2a25-72b7-482c-c252-9b304c75f44e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2 4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}